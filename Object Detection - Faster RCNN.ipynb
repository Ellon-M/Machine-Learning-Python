{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Object Detection - Faster RCNN (PyTorch)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Much of the implementation here is described by the use of pre-defined functions. It's a much cleaner way of conducting this whole process, in my opinion.*","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import  FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n\nimport os\nimport numpy as np\nimport cv2\nimport glob\nimport albumentations as A\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_PATH = '../input/train-test-imgs-orig/Train_Images/Train_Images'\nTEST_PATH = '../input/train-test-imgs-orig/Test_Images (1) (1)/Test_Images'\nPREDICTION_THRES = 0.5\nEPOCHS = 20\nMIN_SIZE = 800\nBATCH_SIZE = 4\nDEBUG = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model():\n    # we will keep the image size to the original 800 for faster training,\n    # larger image size would for sure result in better - more accurate results\n    # however there is a trade off since the training time increases\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, \n                                                                 min_size=MIN_SIZE)\n    # additional background class\n    num_classes = 4\n    # get the input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace pre-trained head with our features head\n    # the head layer will classify the images based on our data input features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Loading images when working with PyTorch is a hassle. If the images are disorganized, creating a custom class for loading the dataset is the way to go. Understandably, if the images are of small quantity, organizing them in folders and using the ImageFolder library would be a much simpler, and less boilerplate solution.*","metadata":{}},{"cell_type":"code","source":"class FruitDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['Image_ID'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        \n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['Image_ID'] == image_id]\n\n        image = cv2.imread(f\"{self.image_dir}/{image_id}.jpg\", cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n    \n        # convert the boxes into x_min, y_min, x_max, y_max format\n        boxes = records[['xmin', 'ymin', 'width', 'height']].values\n        # x_max\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        \n        #y_max\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        # get the area of the bounding boxes\n        # h * w\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # we have only one class - detecting only one object\n        labels = torch.ones((records.shape[1],), dtype=torch.int64)\n        \n        # supposing that all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_ID'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        # apply the image transforms\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.FloatTensor, \n                                                    zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self):\n        return self.image_ids.shape[0]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    This function helps when we have different number of object instances\n    in the batches in the dataset.\n    \"\"\"\n    return tuple(zip(*batch))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for the image transforms\ndef train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        A.RandomRotate90(0.5),\n#         A.MotionBlur(p=0.2),\n#         A.MedianBlur(blur_limit=3, p=0.1),\n        A.Blur(blur_limit=3, p=0.1),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*As for the image transforms, you always really want to have the A.Flip mentioned - at the very least.*","metadata":{}},{"cell_type":"code","source":"# path to the input root directory\nDIR_INPUT = ROOT_PATH\n# read the annotation CSV file\ntrain_df = pd.read_csv(f\"../input/csv-files/Train (6).csv\")\nprint(train_df.head())\nprint(f\"Total number of image IDs (objects) in dataframe: {len(train_df)}\")\n\n# get all the image paths as list\nimage_paths = glob.glob(f\"{DIR_INPUT}/*.jpg\")\nimage_names = []\nfor image_path in image_paths:\n    image_names.append(image_path.split(os.path.sep)[-1].split('.')[0])\nprint(f\"Total number of training images in folder: {len(image_names)}\")\nimage_ids = train_df['Image_ID'].unique()\nprint(f\"Total number of unique train images IDs in dataframe: {len(image_ids)}\")\n# number of images that we want to train out of all the unique images\ntrain_ids = image_names[:] # use all the images for training\ntrain_df = train_df[train_df['Image_ID'].isin(train_ids)]\nprint(f\"Number of image IDs (objects) training on: {len(train_df)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = FruitDataset(train_df, DIR_INPUT, train_transform())\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*The model generally has a good runtime, so using the stochastic gradient decent optimizer won't hurt.*","metadata":{}},{"cell_type":"code","source":"# the computation device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(train_dataloader):\n    model.train()\n    running_loss = 0\n    for i, data in enumerate(train_dataloader):\n        \n        optimizer.zero_grad()\n        images, targets, images_ids = data[0], data[1], data[2]\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n        running_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        if i % 25 == 0:\n            print(f\"Iteration #{i} loss: {loss}\")\n    train_loss = running_loss/len(train_dataloader.dataset)\n    return train_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model():\n    torch.save(model.state_dict(), '/kaggle/working/fasterrcnn_resnet50_fpn.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for the lovers of visualization, like me\ndef visualize():\n    \"\"\"\n    This function will only execute if `DEBUG` is `True` in \n    the third cell\n    \"\"\"\n    images, targets, image_ids = next(iter(train_data_loader))\n    images = list(image for image in images)\n    targets = [{k: v for k, v in t.items()} for t in targets]\n    for i in range(1):\n        boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n        sample = images[i].permute(1,2,0).cpu().numpy()\n        fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n        for box in boxes:\n            cv2.rectangle(sample,\n                        (box[0], box[1]),\n                        (box[2], box[3]),\n                        (220, 0, 0), 3)\n        ax.set_axis_off()\n        plt.imshow(sample)\n        plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# such wonderful code right here...\nimport time\nif DEBUG:\n    visualize()\n    \nnum_epochs = EPOCHS\nfor epoch in range(num_epochs):\n    start = time.time()\n    train_loss = train(train_data_loader)\n    print(f\"Epoch #{epoch} loss: {train_loss}\")   \n    end = time.time()\n    print(f\"Took {(end - start) / 60} minutes for epoch {epoch}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation time\n\nfrom tqdm import tqdm\n# set the computation device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel.load_state_dict(torch.load('/kaggle/working/fasterrcnn_resnet50_fpn.pth'))\n\nDIR_TEST = TEST_PATH\ntest_images = os.listdir(DIR_TEST)\nprint(f\"Validation instances: {len(test_images)}\")\n\nboxes_list = []\nscores_list = []\nimage_paths = []\n\ndetection_threshold = PREDICTION_THRES\nmodel.eval()\nwith torch.no_grad():\n    for i, image in tqdm(enumerate(test_images), total=len(test_images)):\n        orig_image = cv2.imread(f\"{DIR_TEST}/{test_images[i]}\", cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        # make the pixel range between 0 and 1\n        image /= 255.0\n        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n        image = torch.tensor(image, dtype=torch.float).cuda()\n        image = torch.unsqueeze(image, 0)\n        cpu_device = torch.device(\"cpu\")\n        outputs = model(image)\n        \n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        if len(outputs[0]['boxes']) != 0:\n            for counter in range(len(outputs[0]['boxes'])):\n                image_paths.append(f\"{test_images[i]}\")\n                boxes = outputs[0]['boxes'].data.numpy()\n                scores = outputs[0]['scores'].data.numpy()\n                labels = outputs[0]['labels'].data.numpy()\n                boxes = boxes[scores >= detection_threshold].astype(np.float32)\n                scores = scores[scores >= detection_threshold].astype(np.float32)\n                boxes_list.append(boxes)\n                scores_list.append(scores)\n                draw_boxes = boxes.copy()\n                \n                \n            for box in enumerate(draw_boxes):\n                cv2.rectangle(orig_image,\n                            (int(box[0]), int(box[1])),\n                            (int(box[2]), int(box[3])),\n                            (0, 0, 255), 3)\n                cv2.putText(orig_image, pred_classes[box], \n                            (int(box[0]), int(box[1]-5)),\n                            cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), \n                            2, lineType=cv2.LINE_AA)\n            cv2.imwrite(f\"/kaggle/working/{test_images[i]}\", orig_image,)\nprint('TEST PREDICTIONS COMPLETE')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Test Images, with annotated predictions are written to the specified path. Bounding boxes are all appended to the list `boxes_list` and their confidence scores to the list `scores_list`. All predictions took into account the pre-set threshold of 0.5. This value should generally be as high as your data allows it to be, to maximize on precision.*\n\n*Notably, however, if the metric of choice is mAP, as for most Object Detection tasks, having more than the most accurate prediction that the model generates is a better choice. Despite that, the Faster RCNN's most outstanding feature, compared to its counterparts and alternatives, is its fast computational speed.*","metadata":{}}]}